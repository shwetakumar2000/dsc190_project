{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": "\n    <div class=\"bk-root\">\n        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n        <span id=\"1002\">Loading BokehJS ...</span>\n    </div>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "application/javascript": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  var JS_MIME_TYPE = 'application/javascript';\n  var HTML_MIME_TYPE = 'text/html';\n  var EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n  var CLASS_NAME = 'output_bokeh rendered_html';\n\n  /**\n   * Render data to the DOM node\n   */\n  function render(props, node) {\n    var script = document.createElement(\"script\");\n    node.appendChild(script);\n  }\n\n  /**\n   * Handle when an output is cleared or removed\n   */\n  function handleClearOutput(event, handle) {\n    var cell = handle.cell;\n\n    var id = cell.output_area._bokeh_element_id;\n    var server_id = cell.output_area._bokeh_server_id;\n    // Clean up Bokeh references\n    if (id != null && id in Bokeh.index) {\n      Bokeh.index[id].model.document.clear();\n      delete Bokeh.index[id];\n    }\n\n    if (server_id !== undefined) {\n      // Clean up Bokeh references\n      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n      cell.notebook.kernel.execute(cmd, {\n        iopub: {\n          output: function(msg) {\n            var id = msg.content.text.trim();\n            if (id in Bokeh.index) {\n              Bokeh.index[id].model.document.clear();\n              delete Bokeh.index[id];\n            }\n          }\n        }\n      });\n      // Destroy server and session\n      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n      cell.notebook.kernel.execute(cmd);\n    }\n  }\n\n  /**\n   * Handle when a new output is added\n   */\n  function handleAddOutput(event, handle) {\n    var output_area = handle.output_area;\n    var output = handle.output;\n\n    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n    if ((output.output_type != \"display_data\") || (!Object.prototype.hasOwnProperty.call(output.data, EXEC_MIME_TYPE))) {\n      return\n    }\n\n    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n\n    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n      // store reference to embed id on output_area\n      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n    }\n    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n      var bk_div = document.createElement(\"div\");\n      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n      var script_attrs = bk_div.children[0].attributes;\n      for (var i = 0; i < script_attrs.length; i++) {\n        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\n      }\n      // store reference to server id on output_area\n      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n    }\n  }\n\n  function register_renderer(events, OutputArea) {\n\n    function append_mime(data, metadata, element) {\n      // create a DOM node to render to\n      var toinsert = this.create_output_subarea(\n        metadata,\n        CLASS_NAME,\n        EXEC_MIME_TYPE\n      );\n      this.keyboard_manager.register_events(toinsert);\n      // Render to node\n      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n      render(props, toinsert[toinsert.length - 1]);\n      element.append(toinsert);\n      return toinsert\n    }\n\n    /* Handle when an output is cleared or removed */\n    events.on('clear_output.CodeCell', handleClearOutput);\n    events.on('delete.Cell', handleClearOutput);\n\n    /* Handle when a new output is added */\n    events.on('output_added.OutputArea', handleAddOutput);\n\n    /**\n     * Register the mime type and append_mime function with output_area\n     */\n    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n      /* Is output safe? */\n      safe: true,\n      /* Index of renderer in `output_area.display_order` */\n      index: 0\n    });\n  }\n\n  // register the mime type if in Jupyter Notebook environment and previously unregistered\n  if (root.Jupyter !== undefined) {\n    var events = require('base/js/events');\n    var OutputArea = require('notebook/js/outputarea').OutputArea;\n\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n      register_renderer(events, OutputArea);\n    }\n  }\n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  var NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    var el = document.getElementById(\"1002\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error(url) {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (let i = 0; i < css_urls.length; i++) {\n      const url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    const hashes = {\"https://cdn.bokeh.org/bokeh/release/bokeh-2.3.0.min.js\": \"HjagQp6T0/7bxYTAXbLotF1MLAGWmhkY5siA1Gc/pcEgvgRPtMsRn0gQtMwGKiw1\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.3.0.min.js\": \"ZEPPTjL+mdyqgIq+/pl9KTwzji8Kow2NnI3zWY8+sFinWP/SYJ80BnfeJsa45iYj\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.3.0.min.js\": \"exLqv2ACDRIaV7ZK1iL8aGzGYQvKVuT3U2CT7FsQREBxRah6JrkVCoFy0koY1YqV\"};\n\n    for (let i = 0; i < js_urls.length; i++) {\n      const url = js_urls[i];\n      const element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.async = false;\n      element.src = url;\n      if (url in hashes) {\n        element.crossOrigin = \"anonymous\";\n        element.integrity = \"sha384-\" + hashes[url];\n      }\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  \n  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.3.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.3.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.3.0.min.js\"];\n  var css_urls = [];\n  \n\n  var inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    function(Bokeh) {\n    \n    \n    }\n  ];\n\n  function run_inline_js() {\n    \n    if (root.Bokeh !== undefined || force === true) {\n      \n    for (var i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\n    if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      var cell = $(document.getElementById(\"1002\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));",
      "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  var NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    var el = document.getElementById(\"1002\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error(url) {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (let i = 0; i < css_urls.length; i++) {\n      const url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    const hashes = {\"https://cdn.bokeh.org/bokeh/release/bokeh-2.3.0.min.js\": \"HjagQp6T0/7bxYTAXbLotF1MLAGWmhkY5siA1Gc/pcEgvgRPtMsRn0gQtMwGKiw1\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.3.0.min.js\": \"ZEPPTjL+mdyqgIq+/pl9KTwzji8Kow2NnI3zWY8+sFinWP/SYJ80BnfeJsa45iYj\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.3.0.min.js\": \"exLqv2ACDRIaV7ZK1iL8aGzGYQvKVuT3U2CT7FsQREBxRah6JrkVCoFy0koY1YqV\"};\n\n    for (let i = 0; i < js_urls.length; i++) {\n      const url = js_urls[i];\n      const element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.async = false;\n      element.src = url;\n      if (url in hashes) {\n        element.crossOrigin = \"anonymous\";\n        element.integrity = \"sha384-\" + hashes[url];\n      }\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  \n  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.3.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.3.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.3.0.min.js\"];\n  var css_urls = [];\n  \n\n  var inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    function(Bokeh) {\n    \n    \n    }\n  ];\n\n  function run_inline_js() {\n    \n    if (root.Bokeh !== undefined || force === true) {\n      \n    for (var i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\n    if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      var cell = $(document.getElementById(\"1002\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {}
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from time import sleep\n",
    "\n",
    "from pandas_datareader import data as pdr\n",
    "import pandas_ta as ta #commented out for me for now, I (Ben) had some dependency issues\n",
    "\n",
    "import yfinance as yf\n",
    "# import quandl as qd # not used, ignor ples\n",
    "from backtesting import Backtest, Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "yf.pdr_override() # for use with pandas-datareader, optional"
   ]
  },
  {
   "source": [
    "uncomment below if you need to update/generate your monthly and daily csv files."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "**********************]  1 of 1 completed\n",
      "Grabbing DPZ data!\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Grabbing GPC data!\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Grabbing J data!\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Grabbing ABMD data!\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Grabbing K data!\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Grabbing TDY data!\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Grabbing WAB data!\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Grabbing MGM data!\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Grabbing RJF data!\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Grabbing OMC data!\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Grabbing FMC data!\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Grabbing LDOS data!\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Grabbing MAS data!\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Grabbing BIO data!\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Grabbing PFG data!\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Grabbing BF-B data!\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Grabbing RCL data!\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Grabbing CINF data!\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Grabbing POOL data!\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Grabbing UAL data!\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Grabbing HRL data!\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Grabbing BXP data!\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Grabbing SJM data!\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Grabbing FFIV data!\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Grabbing NLOK data!\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Grabbing PKG data!\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Grabbing UDR data!\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Grabbing PHM data!\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Grabbing EVRG data!\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Grabbing DVN data!\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Grabbing WHR data!\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Grabbing JBHT data!\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Grabbing LYV data!\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Grabbing CHRW data!\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Grabbing FBHS data!\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Grabbing DISCK data!\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Grabbing LNT data!\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Grabbing XRAY data!\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Grabbing WYNN data!\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Grabbing TXT data!\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Grabbing CNP data!\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Grabbing LUMN data!\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Grabbing LB data!\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Grabbing WRK data!\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Grabbing HAS data!\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Grabbing ATO data!\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Grabbing L data!\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Grabbing LW data!\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Grabbing JKHY data!\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Grabbing HWM data!\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Grabbing HST data!\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Grabbing FANG data!\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Grabbing AAL data!\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Grabbing TPR data!\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Grabbing PWR data!\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Grabbing FOXA data!\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Grabbing BWA data!\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Grabbing MOS data!\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Grabbing AAP data!\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Grabbing LKQ data!\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Grabbing CBOE data!\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Grabbing MHK data!\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Grabbing SNA data!\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Grabbing ALLE data!\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Grabbing HSIC data!\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Grabbing UHS data!\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Grabbing NRG data!\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Grabbing LNC data!\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Grabbing WU data!\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Grabbing IPG data!\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Grabbing CF data!\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Grabbing RE data!\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Grabbing WRB data!\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Grabbing IRM data!\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Grabbing CPB data!\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Grabbing NWSA data!\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Grabbing CMA data!\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Grabbing IPGP data!\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Grabbing PNR data!\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Grabbing GL data!\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Grabbing NWL data!\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Grabbing IVZ data!\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Grabbing NI data!\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Grabbing PNW data!\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Grabbing RHI data!\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Grabbing ZION data!\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Grabbing TAP data!\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Grabbing NLSN data!\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Grabbing REG data!\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Grabbing ROL data!\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Grabbing JNPR data!\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Grabbing AOS data!\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Grabbing DISH data!\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Grabbing NCLH data!\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Grabbing DVA data!\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Grabbing BEN data!\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Grabbing KIM data!\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Grabbing DISCA data!\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Grabbing MRO data!\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Grabbing AIZ data!\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Grabbing ALK data!\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Grabbing COG data!\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Grabbing FLIR data!\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Grabbing HII data!\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Grabbing APA data!\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Grabbing FRT data!\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Grabbing PVH data!\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Grabbing SEE data!\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Grabbing PBCT data!\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Grabbing DXC data!\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Grabbing HBI data!\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Grabbing PRGO data!\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Grabbing VNO data!\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Grabbing NOV data!\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Grabbing LEG data!\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Grabbing RL data!\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Grabbing UNM data!\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Grabbing FLS data!\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Grabbing HFC data!\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Grabbing GPS data!\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Grabbing FOX data!\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Grabbing SLG data!\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Grabbing VNT data!\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Grabbing UAA data!\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Grabbing XRX data!\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Grabbing UA data!\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Grabbing NWS data!\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "# ticker = pd.read_csv('ticker.csv')['Ticker']\n",
    "# tickers = ticker.to_list() # This is a list of all tickers in the SP500\n",
    "# tickers = [x.replace('.','-') for x in tickers] # yahoo has '-' instead of '.' for tickers, eg BRK.B\n",
    "\n",
    "# ## below is how I got monthly and daily pandas dataframes of all stocks in one huge dict.  \n",
    "# monthly_sp500 = {}\n",
    "# daily_sp500 = {}\n",
    "# for tkr in tickers: # run all 500 at your own risk, it takes a while lol\n",
    "# # for tkr in tickers[:10]:\n",
    "#     print('Grabbing ' + tkr + \" data!\")\n",
    "#     monthly_sp500[tkr] = pdr.get_data_yahoo(tkr, start=\"2010-01-01\", interval = \"1mo\")\n",
    "#     daily_sp500[tkr] = pdr.get_data_yahoo(tkr,start=\"2010-01-01\")\n",
    "#     sleep(.1) # not planning on ddos-ing yahoo today\n",
    "# monthly_sp500['AAPL'] # take AAPL, for example\n",
    "\n",
    "# ## everything in one stupid large dataframe\n",
    "# sp500m = monthly_sp500[tickers[0]] # monthly\n",
    "# sp500m['Name'] = tickers[0]\n",
    "\n",
    "# sp500d = daily_sp500[tickers[0]] # daily\n",
    "# sp500d['Name'] = tickers[0]\n",
    "\n",
    "# # for tkr in tickers[1:10]:\n",
    "# for tkr in tickers[1:]:\n",
    "#     df1 = monthly_sp500[tkr] # monthly\n",
    "#     df1['Name'] = tkr\n",
    "#     sp500m = sp500m.append(df1)\n",
    "\n",
    "#     df2 = daily_sp500[tkr] # daily\n",
    "#     df2['Name'] = tkr\n",
    "#     sp500d = sp500d.append(df2)\n",
    "\n",
    "# sp500m.to_csv('SP500_monthly.csv') # run/uncomment some of these to save these to csv\n",
    "# sp500d.to_csv('SP500_daily.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "               Date       Open       High        Low      Close  Adj Close  \\\n",
       "0        2010-01-04   7.622500   7.660714   7.585000   7.643214   6.583586   \n",
       "1        2010-01-05   7.664286   7.699643   7.616071   7.656429   6.594968   \n",
       "2        2010-01-06   7.656429   7.686786   7.526786   7.534643   6.490066   \n",
       "3        2010-01-07   7.562500   7.571429   7.466071   7.520714   6.478067   \n",
       "4        2010-01-08   7.510714   7.571429   7.466429   7.570714   6.521136   \n",
       "...             ...        ...        ...        ...        ...        ...   \n",
       "1365206  2021-03-04  23.020000  23.330000  22.389999  22.770000  22.770000   \n",
       "1365207  2021-03-05  22.940001  23.549999  22.510000  23.500000  23.500000   \n",
       "1365208  2021-03-08  23.500000  23.760000  22.980000  23.230000  23.230000   \n",
       "1365209  2021-03-09  24.000000  24.459999  23.389999  23.799999  23.799999   \n",
       "1365210  2021-03-10  23.860001  24.770000  23.709999  24.370001  24.370001   \n",
       "\n",
       "              Volume  Name  \n",
       "0        493729600.0  AAPL  \n",
       "1        601904800.0  AAPL  \n",
       "2        552160000.0  AAPL  \n",
       "3        477131200.0  AAPL  \n",
       "4        447610800.0  AAPL  \n",
       "...              ...   ...  \n",
       "1365206     345900.0   NWS  \n",
       "1365207     515500.0   NWS  \n",
       "1365208     754500.0   NWS  \n",
       "1365209     632600.0   NWS  \n",
       "1365210     787033.0   NWS  \n",
       "\n",
       "[1365210 rows x 8 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Date</th>\n      <th>Open</th>\n      <th>High</th>\n      <th>Low</th>\n      <th>Close</th>\n      <th>Adj Close</th>\n      <th>Volume</th>\n      <th>Name</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2010-01-04</td>\n      <td>7.622500</td>\n      <td>7.660714</td>\n      <td>7.585000</td>\n      <td>7.643214</td>\n      <td>6.583586</td>\n      <td>493729600.0</td>\n      <td>AAPL</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2010-01-05</td>\n      <td>7.664286</td>\n      <td>7.699643</td>\n      <td>7.616071</td>\n      <td>7.656429</td>\n      <td>6.594968</td>\n      <td>601904800.0</td>\n      <td>AAPL</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2010-01-06</td>\n      <td>7.656429</td>\n      <td>7.686786</td>\n      <td>7.526786</td>\n      <td>7.534643</td>\n      <td>6.490066</td>\n      <td>552160000.0</td>\n      <td>AAPL</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2010-01-07</td>\n      <td>7.562500</td>\n      <td>7.571429</td>\n      <td>7.466071</td>\n      <td>7.520714</td>\n      <td>6.478067</td>\n      <td>477131200.0</td>\n      <td>AAPL</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2010-01-08</td>\n      <td>7.510714</td>\n      <td>7.571429</td>\n      <td>7.466429</td>\n      <td>7.570714</td>\n      <td>6.521136</td>\n      <td>447610800.0</td>\n      <td>AAPL</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1365206</th>\n      <td>2021-03-04</td>\n      <td>23.020000</td>\n      <td>23.330000</td>\n      <td>22.389999</td>\n      <td>22.770000</td>\n      <td>22.770000</td>\n      <td>345900.0</td>\n      <td>NWS</td>\n    </tr>\n    <tr>\n      <th>1365207</th>\n      <td>2021-03-05</td>\n      <td>22.940001</td>\n      <td>23.549999</td>\n      <td>22.510000</td>\n      <td>23.500000</td>\n      <td>23.500000</td>\n      <td>515500.0</td>\n      <td>NWS</td>\n    </tr>\n    <tr>\n      <th>1365208</th>\n      <td>2021-03-08</td>\n      <td>23.500000</td>\n      <td>23.760000</td>\n      <td>22.980000</td>\n      <td>23.230000</td>\n      <td>23.230000</td>\n      <td>754500.0</td>\n      <td>NWS</td>\n    </tr>\n    <tr>\n      <th>1365209</th>\n      <td>2021-03-09</td>\n      <td>24.000000</td>\n      <td>24.459999</td>\n      <td>23.389999</td>\n      <td>23.799999</td>\n      <td>23.799999</td>\n      <td>632600.0</td>\n      <td>NWS</td>\n    </tr>\n    <tr>\n      <th>1365210</th>\n      <td>2021-03-10</td>\n      <td>23.860001</td>\n      <td>24.770000</td>\n      <td>23.709999</td>\n      <td>24.370001</td>\n      <td>24.370001</td>\n      <td>787033.0</td>\n      <td>NWS</td>\n    </tr>\n  </tbody>\n</table>\n<p>1365210 rows × 8 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 73
    }
   ],
   "source": [
    "# df = pd.read_csv('SP500_monthly.csv').infer_objects().dropna()\n",
    "df = pd.read_csv('SP500_daily.csv').infer_objects().dropna()\n",
    "# df = sp500m.infer_objects().dropna()#.reset_index() # dataframe of all we basically want, OHLC data w/ adjusted close. didnt set any ind\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# j=1\n",
    "# k=2\n",
    "# # the 'midpoints' for each observe and hold period. midpoint in this context is where we switch from the observation period len=j to the holding period len=k\n",
    "# j_end = pd.date_range(st+pd.DateOffset(months=j*3), et, freq=str(k*3)+\"MS\")\n",
    "# j_end.strftime(\"%Y-%m-%d\").to_list()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.date_range(st+pd.DateOffset(days=j), et)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "204\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "     j_start   midpoint      k_end\n",
       "0 2010-01-04 2010-01-14 2010-02-03\n",
       "1 2010-01-24 2010-02-03 2010-02-23\n",
       "2 2010-02-13 2010-02-23 2010-03-15\n",
       "3 2010-03-05 2010-03-15 2010-04-04\n",
       "4 2010-03-25 2010-04-04 2010-04-24"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>j_start</th>\n      <th>midpoint</th>\n      <th>k_end</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2010-01-04</td>\n      <td>2010-01-14</td>\n      <td>2010-02-03</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2010-01-24</td>\n      <td>2010-02-03</td>\n      <td>2010-02-23</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2010-02-13</td>\n      <td>2010-02-23</td>\n      <td>2010-03-15</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2010-03-05</td>\n      <td>2010-03-15</td>\n      <td>2010-04-04</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2010-03-25</td>\n      <td>2010-04-04</td>\n      <td>2010-04-24</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "# def generate_momentum_returns(j=25,k=50):\n",
    "def generate_daily_timetable(j=10, k=20):\n",
    "    '''aims to generate the MONTHLY table of times for each observe/hold period. The midpoint in this context is where we switch from the observation period len=j to the holding period len=k'''\n",
    "    # TODO FINISAH FISODJNF\n",
    "    j_end = pd.date_range(pd.to_datetime(df.Date.min())+pd.DateOffset(days=j), pd.to_datetime(df.Date.max()), freq=str(k)+\"D\") # the 'midpoints' for each observe and hold period. \n",
    "    j_start = j_end + pd.DateOffset(days=-j) # based on the midpoint, get the start point for each observe and hold period\n",
    "    k_end = j_end + pd.DateOffset(days=k) # based on the midpoint, get the end point for each observe and hold period\n",
    "    timetable = pd.DataFrame({'j_start': j_start, 'midpoint': j_end, 'k_end': k_end}).infer_objects()\n",
    "    return timetable, j, k\n",
    "timedf, j, k = generate_daily_timetable()\n",
    "print(timedf.shape[0])\n",
    "timedf.head() ##NOTE THE K_END VALUE IS NO LONGER EQUAL TO THE NEXT MIDPOINT BUT OFFSET BY 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "               Date       Open       High        Low      Close  Adj Close  \\\n",
       "0        2010-01-04   7.622500   7.660714   7.585000   7.643214   6.583586   \n",
       "1        2010-01-05   7.664286   7.699643   7.616071   7.656429   6.594968   \n",
       "2        2010-01-06   7.656429   7.686786   7.526786   7.534643   6.490066   \n",
       "3        2010-01-07   7.562500   7.571429   7.466071   7.520714   6.478067   \n",
       "4        2010-01-08   7.510714   7.571429   7.466429   7.570714   6.521136   \n",
       "...             ...        ...        ...        ...        ...        ...   \n",
       "1365206  2021-03-04  23.020000  23.330000  22.389999  22.770000  22.770000   \n",
       "1365207  2021-03-05  22.940001  23.549999  22.510000  23.500000  23.500000   \n",
       "1365208  2021-03-08  23.500000  23.760000  22.980000  23.230000  23.230000   \n",
       "1365209  2021-03-09  24.000000  24.459999  23.389999  23.799999  23.799999   \n",
       "1365210  2021-03-10  23.860001  24.770000  23.709999  24.370001  24.370001   \n",
       "\n",
       "              Volume  Name  \n",
       "0        493729600.0  AAPL  \n",
       "1        601904800.0  AAPL  \n",
       "2        552160000.0  AAPL  \n",
       "3        477131200.0  AAPL  \n",
       "4        447610800.0  AAPL  \n",
       "...              ...   ...  \n",
       "1365206     345900.0   NWS  \n",
       "1365207     515500.0   NWS  \n",
       "1365208     754500.0   NWS  \n",
       "1365209     632600.0   NWS  \n",
       "1365210     787033.0   NWS  \n",
       "\n",
       "[1365210 rows x 8 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Date</th>\n      <th>Open</th>\n      <th>High</th>\n      <th>Low</th>\n      <th>Close</th>\n      <th>Adj Close</th>\n      <th>Volume</th>\n      <th>Name</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2010-01-04</td>\n      <td>7.622500</td>\n      <td>7.660714</td>\n      <td>7.585000</td>\n      <td>7.643214</td>\n      <td>6.583586</td>\n      <td>493729600.0</td>\n      <td>AAPL</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2010-01-05</td>\n      <td>7.664286</td>\n      <td>7.699643</td>\n      <td>7.616071</td>\n      <td>7.656429</td>\n      <td>6.594968</td>\n      <td>601904800.0</td>\n      <td>AAPL</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2010-01-06</td>\n      <td>7.656429</td>\n      <td>7.686786</td>\n      <td>7.526786</td>\n      <td>7.534643</td>\n      <td>6.490066</td>\n      <td>552160000.0</td>\n      <td>AAPL</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2010-01-07</td>\n      <td>7.562500</td>\n      <td>7.571429</td>\n      <td>7.466071</td>\n      <td>7.520714</td>\n      <td>6.478067</td>\n      <td>477131200.0</td>\n      <td>AAPL</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2010-01-08</td>\n      <td>7.510714</td>\n      <td>7.571429</td>\n      <td>7.466429</td>\n      <td>7.570714</td>\n      <td>6.521136</td>\n      <td>447610800.0</td>\n      <td>AAPL</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1365206</th>\n      <td>2021-03-04</td>\n      <td>23.020000</td>\n      <td>23.330000</td>\n      <td>22.389999</td>\n      <td>22.770000</td>\n      <td>22.770000</td>\n      <td>345900.0</td>\n      <td>NWS</td>\n    </tr>\n    <tr>\n      <th>1365207</th>\n      <td>2021-03-05</td>\n      <td>22.940001</td>\n      <td>23.549999</td>\n      <td>22.510000</td>\n      <td>23.500000</td>\n      <td>23.500000</td>\n      <td>515500.0</td>\n      <td>NWS</td>\n    </tr>\n    <tr>\n      <th>1365208</th>\n      <td>2021-03-08</td>\n      <td>23.500000</td>\n      <td>23.760000</td>\n      <td>22.980000</td>\n      <td>23.230000</td>\n      <td>23.230000</td>\n      <td>754500.0</td>\n      <td>NWS</td>\n    </tr>\n    <tr>\n      <th>1365209</th>\n      <td>2021-03-09</td>\n      <td>24.000000</td>\n      <td>24.459999</td>\n      <td>23.389999</td>\n      <td>23.799999</td>\n      <td>23.799999</td>\n      <td>632600.0</td>\n      <td>NWS</td>\n    </tr>\n    <tr>\n      <th>1365210</th>\n      <td>2021-03-10</td>\n      <td>23.860001</td>\n      <td>24.770000</td>\n      <td>23.709999</td>\n      <td>24.370001</td>\n      <td>24.370001</td>\n      <td>787033.0</td>\n      <td>NWS</td>\n    </tr>\n  </tbody>\n</table>\n<p>1365210 rows × 8 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 72
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "               Date        Open        High         Low       Close  \\\n",
       "0        2010-01-14    7.503929    7.516429    7.465000    7.479643   \n",
       "1        2010-01-14   30.309999   31.100000   30.260000   30.959999   \n",
       "2        2010-01-14  129.139999  130.380005  126.400002  127.349998   \n",
       "3        2010-01-14  292.242249  297.397400  291.696686  295.220215   \n",
       "4        2010-01-14  290.859772  295.990570  290.316833  293.823669   \n",
       "...             ...         ...         ...         ...         ...   \n",
       "1430470  2021-03-09   31.990000   32.279999   31.010000   31.059999   \n",
       "1430471  2021-03-09   23.129999   23.139999   22.400000   22.410000   \n",
       "1430472  2021-03-09   26.680000   26.940001   26.320000   26.510000   \n",
       "1430473  2021-03-09   19.240000   19.250000   18.692200   18.709999   \n",
       "1430474  2021-03-09   24.000000   24.459999   23.389999   23.799999   \n",
       "\n",
       "          Adj Close       Volume   Name       date  period  \n",
       "0          6.442690  432894000.0   AAPL 2010-01-14       0  \n",
       "1         24.057743   63228100.0   MSFT 2010-01-14       0  \n",
       "2        127.349998    9774900.0   AMZN 2010-01-14       0  \n",
       "3        295.220215    8471720.0  GOOGL 2010-01-14       0  \n",
       "4        293.823669    8511986.0   GOOG 2010-01-14       0  \n",
       "...             ...          ...    ...        ...     ...  \n",
       "1430470   31.059999    2735100.0    VNT 2021-03-09     406  \n",
       "1430471   22.410000    4301100.0    UAA 2021-03-09     406  \n",
       "1430472   26.510000    2113200.0    XRX 2021-03-09     406  \n",
       "1430473   18.709999    2110296.0     UA 2021-03-09     406  \n",
       "1430474   23.799999     632600.0    NWS 2021-03-09     406  \n",
       "\n",
       "[1430475 rows x 10 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Date</th>\n      <th>Open</th>\n      <th>High</th>\n      <th>Low</th>\n      <th>Close</th>\n      <th>Adj Close</th>\n      <th>Volume</th>\n      <th>Name</th>\n      <th>date</th>\n      <th>period</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2010-01-14</td>\n      <td>7.503929</td>\n      <td>7.516429</td>\n      <td>7.465000</td>\n      <td>7.479643</td>\n      <td>6.442690</td>\n      <td>432894000.0</td>\n      <td>AAPL</td>\n      <td>2010-01-14</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2010-01-14</td>\n      <td>30.309999</td>\n      <td>31.100000</td>\n      <td>30.260000</td>\n      <td>30.959999</td>\n      <td>24.057743</td>\n      <td>63228100.0</td>\n      <td>MSFT</td>\n      <td>2010-01-14</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2010-01-14</td>\n      <td>129.139999</td>\n      <td>130.380005</td>\n      <td>126.400002</td>\n      <td>127.349998</td>\n      <td>127.349998</td>\n      <td>9774900.0</td>\n      <td>AMZN</td>\n      <td>2010-01-14</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2010-01-14</td>\n      <td>292.242249</td>\n      <td>297.397400</td>\n      <td>291.696686</td>\n      <td>295.220215</td>\n      <td>295.220215</td>\n      <td>8471720.0</td>\n      <td>GOOGL</td>\n      <td>2010-01-14</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2010-01-14</td>\n      <td>290.859772</td>\n      <td>295.990570</td>\n      <td>290.316833</td>\n      <td>293.823669</td>\n      <td>293.823669</td>\n      <td>8511986.0</td>\n      <td>GOOG</td>\n      <td>2010-01-14</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1430470</th>\n      <td>2021-03-09</td>\n      <td>31.990000</td>\n      <td>32.279999</td>\n      <td>31.010000</td>\n      <td>31.059999</td>\n      <td>31.059999</td>\n      <td>2735100.0</td>\n      <td>VNT</td>\n      <td>2021-03-09</td>\n      <td>406</td>\n    </tr>\n    <tr>\n      <th>1430471</th>\n      <td>2021-03-09</td>\n      <td>23.129999</td>\n      <td>23.139999</td>\n      <td>22.400000</td>\n      <td>22.410000</td>\n      <td>22.410000</td>\n      <td>4301100.0</td>\n      <td>UAA</td>\n      <td>2021-03-09</td>\n      <td>406</td>\n    </tr>\n    <tr>\n      <th>1430472</th>\n      <td>2021-03-09</td>\n      <td>26.680000</td>\n      <td>26.940001</td>\n      <td>26.320000</td>\n      <td>26.510000</td>\n      <td>26.510000</td>\n      <td>2113200.0</td>\n      <td>XRX</td>\n      <td>2021-03-09</td>\n      <td>406</td>\n    </tr>\n    <tr>\n      <th>1430473</th>\n      <td>2021-03-09</td>\n      <td>19.240000</td>\n      <td>19.250000</td>\n      <td>18.692200</td>\n      <td>18.709999</td>\n      <td>18.709999</td>\n      <td>2110296.0</td>\n      <td>UA</td>\n      <td>2021-03-09</td>\n      <td>406</td>\n    </tr>\n    <tr>\n      <th>1430474</th>\n      <td>2021-03-09</td>\n      <td>24.000000</td>\n      <td>24.459999</td>\n      <td>23.389999</td>\n      <td>23.799999</td>\n      <td>23.799999</td>\n      <td>632600.0</td>\n      <td>NWS</td>\n      <td>2021-03-09</td>\n      <td>406</td>\n    </tr>\n  </tbody>\n</table>\n<p>1430475 rows × 10 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "from itertools import chain\n",
    "def get_daily_j(id=-1):\n",
    "    jtimes = pd.DataFrame(list(chain.from_iterable(pd.date_range(timedf[\"j_start\"],timedf[\"midpoint\"]) for _,timedf in timedf.iterrows())), columns=(\"date\",))\n",
    "    jtimes['date'] = jtimes['date'].astype('datetime64[ns]')\n",
    "    jtimes['period'] = (jtimes['date'].isin(timedf['midpoint'])).shift(1).cumsum().fillna(0).astype(int)\n",
    "    df_ = df.copy()\n",
    "    df_['date'] = pd.to_datetime(df_['Date'])\n",
    "    df_ = df_.merge(jtimes, how='inner', on='date')\n",
    "    if id==-1: \n",
    "        return df_ # this filters out periods that dont end prettily\n",
    "    else:\n",
    "        return df_[df_['period']==id].drop('period', axis=1)\n",
    "get_j_df = get_daily_j\n",
    "def get_daily_k(id=-1):\n",
    "    ktimes = pd.DataFrame(list(chain.from_iterable(pd.date_range(timedf[\"midpoint\"],timedf[\"k_end\"]) for _,timedf in timedf.iterrows())), columns=(\"date\",))\n",
    "    ktimes['date'] = ktimes['date'].astype('datetime64[ns]')\n",
    "    ktimes['period'] = (ktimes['date'].isin(timedf['k_end'])).shift(1).cumsum().fillna(0).astype(int)\n",
    "    df_ = df.copy()\n",
    "    df_['date'] = pd.to_datetime(df_['Date'])\n",
    "    df_ = df_.merge(ktimes, how='inner', on='date')\n",
    "    if id==-1: \n",
    "        return df_ # this filters out periods that dont end prettily\n",
    "    else:\n",
    "        return df_[df_['period']==id].drop('period', axis=1)\n",
    "get_daily_k()#.period.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "              Date        Open        High         Low       Close  \\\n",
       "0       2010-01-04    7.622500    7.660714    7.585000    7.643214   \n",
       "1       2010-01-04   30.620001   31.100000   30.590000   30.950001   \n",
       "2       2010-01-04  136.250000  136.610001  133.139999  133.899994   \n",
       "3       2010-01-04  313.788788  315.070068  312.432434  313.688690   \n",
       "4       2010-01-04  312.304413  313.579620  310.954468  312.204773   \n",
       "...            ...         ...         ...         ...         ...   \n",
       "750681  2021-02-25   32.490002   33.110001   31.750000   31.910000   \n",
       "750682  2021-02-25   21.940001   22.620001   21.490000   21.590000   \n",
       "750683  2021-02-25   25.309999   25.790001   24.920000   25.350000   \n",
       "750684  2021-02-25   18.280001   18.870001   18.020000   18.100000   \n",
       "750685  2021-02-25   23.350000   23.540001   22.629999   22.650000   \n",
       "\n",
       "         Adj Close       Volume   Name       date  period  \n",
       "0         6.583586  493729600.0   AAPL 2010-01-04       0  \n",
       "1        24.049969   38409100.0   MSFT 2010-01-04       0  \n",
       "2       133.899994    7599900.0   AMZN 2010-01-04       0  \n",
       "3       313.688690    3908488.0  GOOGL 2010-01-04       0  \n",
       "4       312.204773    3927065.0   GOOG 2010-01-04       0  \n",
       "...            ...          ...    ...        ...     ...  \n",
       "750681   31.910000    1604200.0    VNT 2021-02-25     203  \n",
       "750682   21.590000    6187100.0    UAA 2021-02-25     203  \n",
       "750683   25.350000    3828900.0    XRX 2021-02-25     203  \n",
       "750684   18.100000    2355480.0     UA 2021-02-25     203  \n",
       "750685   22.650000     383000.0    NWS 2021-02-25     203  \n",
       "\n",
       "[750686 rows x 10 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Date</th>\n      <th>Open</th>\n      <th>High</th>\n      <th>Low</th>\n      <th>Close</th>\n      <th>Adj Close</th>\n      <th>Volume</th>\n      <th>Name</th>\n      <th>date</th>\n      <th>period</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2010-01-04</td>\n      <td>7.622500</td>\n      <td>7.660714</td>\n      <td>7.585000</td>\n      <td>7.643214</td>\n      <td>6.583586</td>\n      <td>493729600.0</td>\n      <td>AAPL</td>\n      <td>2010-01-04</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2010-01-04</td>\n      <td>30.620001</td>\n      <td>31.100000</td>\n      <td>30.590000</td>\n      <td>30.950001</td>\n      <td>24.049969</td>\n      <td>38409100.0</td>\n      <td>MSFT</td>\n      <td>2010-01-04</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2010-01-04</td>\n      <td>136.250000</td>\n      <td>136.610001</td>\n      <td>133.139999</td>\n      <td>133.899994</td>\n      <td>133.899994</td>\n      <td>7599900.0</td>\n      <td>AMZN</td>\n      <td>2010-01-04</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2010-01-04</td>\n      <td>313.788788</td>\n      <td>315.070068</td>\n      <td>312.432434</td>\n      <td>313.688690</td>\n      <td>313.688690</td>\n      <td>3908488.0</td>\n      <td>GOOGL</td>\n      <td>2010-01-04</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2010-01-04</td>\n      <td>312.304413</td>\n      <td>313.579620</td>\n      <td>310.954468</td>\n      <td>312.204773</td>\n      <td>312.204773</td>\n      <td>3927065.0</td>\n      <td>GOOG</td>\n      <td>2010-01-04</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>750681</th>\n      <td>2021-02-25</td>\n      <td>32.490002</td>\n      <td>33.110001</td>\n      <td>31.750000</td>\n      <td>31.910000</td>\n      <td>31.910000</td>\n      <td>1604200.0</td>\n      <td>VNT</td>\n      <td>2021-02-25</td>\n      <td>203</td>\n    </tr>\n    <tr>\n      <th>750682</th>\n      <td>2021-02-25</td>\n      <td>21.940001</td>\n      <td>22.620001</td>\n      <td>21.490000</td>\n      <td>21.590000</td>\n      <td>21.590000</td>\n      <td>6187100.0</td>\n      <td>UAA</td>\n      <td>2021-02-25</td>\n      <td>203</td>\n    </tr>\n    <tr>\n      <th>750683</th>\n      <td>2021-02-25</td>\n      <td>25.309999</td>\n      <td>25.790001</td>\n      <td>24.920000</td>\n      <td>25.350000</td>\n      <td>25.350000</td>\n      <td>3828900.0</td>\n      <td>XRX</td>\n      <td>2021-02-25</td>\n      <td>203</td>\n    </tr>\n    <tr>\n      <th>750684</th>\n      <td>2021-02-25</td>\n      <td>18.280001</td>\n      <td>18.870001</td>\n      <td>18.020000</td>\n      <td>18.100000</td>\n      <td>18.100000</td>\n      <td>2355480.0</td>\n      <td>UA</td>\n      <td>2021-02-25</td>\n      <td>203</td>\n    </tr>\n    <tr>\n      <th>750685</th>\n      <td>2021-02-25</td>\n      <td>23.350000</td>\n      <td>23.540001</td>\n      <td>22.629999</td>\n      <td>22.650000</td>\n      <td>22.650000</td>\n      <td>383000.0</td>\n      <td>NWS</td>\n      <td>2021-02-25</td>\n      <td>203</td>\n    </tr>\n  </tbody>\n</table>\n<p>750686 rows × 10 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "get_daily_j()#.period.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_timetable(j=1, k=2):\n",
    "#     '''aims to generate the MONTHLY table of times for each observe/hold period. The midpoint in this context is where we switch from the observation period len=j to the holding period len=k'''\n",
    "    \n",
    "#     j_end = pd.date_range(st+pd.DateOffset(months=j*3), et, freq=str(k*3)+\"MS\") # the 'midpoints' for each observe and hold period. \n",
    "#     j_start = j_end + pd.DateOffset(months=-j*3) # based on the midpoint, get the start point for each observe and hold period\n",
    "#     k_end = j_end + pd.DateOffset(months=k*3) # based on the midpoint, get the end point for each observe and hold period\n",
    "#     timetable = pd.DataFrame({'j_start': j_start, 'midpoint': j_end, 'k_end': k_end}).infer_objects()\n",
    "#     return timetable, j, k\n",
    "# timedf, j, k = generate_timetable(3, 2)\n",
    "# timedf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0          2010-01-04\n",
       "1          2010-01-05\n",
       "2          2010-01-06\n",
       "3          2010-01-07\n",
       "4          2010-01-08\n",
       "              ...    \n",
       "1364701    2021-03-03\n",
       "1364702    2021-03-04\n",
       "1364703    2021-03-05\n",
       "1364704    2021-03-08\n",
       "1364705    2021-03-09\n",
       "Name: Date, Length: 1364705, dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "df['Date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "midpoint = timedf.iloc[0].midpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def get_j_df(id=-1, j=j):\n",
    "#     '''this should spit out a dataframe of data in the observational period j given specific id or midpoint value. \n",
    "#     if unspecified it throws all of them at you with an extra identifying column \"periods\"'''\n",
    "#     if (id==-1): # not really sure why we need this but ill include it. this adds a sector column to the data for future filtering purposes if needed\n",
    "#         df_i = pd.DataFrame(columns=df.columns.to_list()+['period']) #dummy empty df\n",
    "#         for i in timedf.index: \n",
    "#             # print(timedf.iloc[i].j_start,timedf.iloc[i].midpoint)\n",
    "#             df_ = df[(timedf.iloc[i].j_start <= pd.to_datetime(df['Date']))&(pd.to_datetime(df['Date']) <= timedf.iloc[i].midpoint)] # gets dates btw start and midpt\n",
    "#             df_['period'] = i\n",
    "#             df_i = pd.concat([df_i, df_])\n",
    "\n",
    "#         return df_i\n",
    "#     return df[(timedf.iloc[id].j_start <= pd.to_datetime(df['Date']))&(pd.to_datetime(df['Date']) <= timedf.iloc[id].midpoint)] # gets dates btw start and midpt from table\n",
    "# get_j_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_k_df(id=-1, k=k):\n",
    "#     '''this should spit out a dataframe of data in the holding period k given specific id or midpoint value. \n",
    "#     if unspecified it throws all of them at you with an extra identifying column \"periods\"'''\n",
    "#     if (id==-1): # we can decide if this is useful later this adds a sector column to the data for future filtering purposes if needed\n",
    "#         df_i = pd.DataFrame(columns=df.columns.to_list()+['period']) #dummy empty df\n",
    "#         for i in timedf.index: \n",
    "#             # print(timedf.iloc[i].j_start,timedf.iloc[i].midpoint)\n",
    "#             df_ = df[(timedf.iloc[i].midpoint <= pd.to_datetime(df['Date']))&(pd.to_datetime(df['Date']) <= timedf.iloc[i].k_end)] # gets dates btw midpt and end\n",
    "#             df_['period'] = i\n",
    "#             df_i = pd.concat([df_i, df_])\n",
    "\n",
    "#         return df_i\n",
    "#     return df[(timedf.iloc[id].midpoint <= pd.to_datetime(df['Date']))&(pd.to_datetime(df['Date']) <= timedf.iloc[id].k_end)] # gets dates btw midpt and end\n",
    "# get_k_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis: using the above functions for building portfolios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "      j_start   midpoint      k_end\n",
       "0  2010-01-04 2010-02-03 2010-04-04\n",
       "1  2010-03-05 2010-04-04 2010-06-03\n",
       "2  2010-05-04 2010-06-03 2010-08-02\n",
       "3  2010-07-03 2010-08-02 2010-10-01\n",
       "4  2010-09-01 2010-10-01 2010-11-30\n",
       "..        ...        ...        ...\n",
       "63 2020-05-11 2020-06-10 2020-08-09\n",
       "64 2020-07-10 2020-08-09 2020-10-08\n",
       "65 2020-09-08 2020-10-08 2020-12-07\n",
       "66 2020-11-07 2020-12-07 2021-02-05\n",
       "67 2021-01-06 2021-02-05 2021-04-06\n",
       "\n",
       "[68 rows x 3 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>j_start</th>\n      <th>midpoint</th>\n      <th>k_end</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2010-01-04</td>\n      <td>2010-02-03</td>\n      <td>2010-04-04</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2010-03-05</td>\n      <td>2010-04-04</td>\n      <td>2010-06-03</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2010-05-04</td>\n      <td>2010-06-03</td>\n      <td>2010-08-02</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2010-07-03</td>\n      <td>2010-08-02</td>\n      <td>2010-10-01</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2010-09-01</td>\n      <td>2010-10-01</td>\n      <td>2010-11-30</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>63</th>\n      <td>2020-05-11</td>\n      <td>2020-06-10</td>\n      <td>2020-08-09</td>\n    </tr>\n    <tr>\n      <th>64</th>\n      <td>2020-07-10</td>\n      <td>2020-08-09</td>\n      <td>2020-10-08</td>\n    </tr>\n    <tr>\n      <th>65</th>\n      <td>2020-09-08</td>\n      <td>2020-10-08</td>\n      <td>2020-12-07</td>\n    </tr>\n    <tr>\n      <th>66</th>\n      <td>2020-11-07</td>\n      <td>2020-12-07</td>\n      <td>2021-02-05</td>\n    </tr>\n    <tr>\n      <th>67</th>\n      <td>2021-01-06</td>\n      <td>2021-02-05</td>\n      <td>2021-04-06</td>\n    </tr>\n  </tbody>\n</table>\n<p>68 rows × 3 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 99
    }
   ],
   "source": [
    "timedf,j,k = generate_daily_timetable(j=30, k=60) # with j as 3 and k as 2\n",
    "timedf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "            Date       Open       High        Low      Close  Adj Close  \\\n",
       "Name                                                                      \n",
       "ZION  2010-02-03  19.770000  19.780001  18.459999  18.620001  16.271383   \n",
       "LYV   2010-02-03  11.410000  11.900000  11.410000  11.890000  11.890000   \n",
       "HBAN  2010-02-03   4.880000   4.900000   4.630000   4.710000   3.466631   \n",
       "DPZ   2010-02-03  11.110000  11.200000  11.030000  11.080000   9.420688   \n",
       "LEN   2010-02-03  16.096361  16.243855  15.644051  15.988201  15.068360   \n",
       "...          ...        ...        ...        ...        ...        ...   \n",
       "HWM   2010-02-03  30.584707  31.236881  30.202398  30.337332  27.653538   \n",
       "AMD   2010-02-03   7.830000   7.960000   7.760000   7.810000   7.810000   \n",
       "AIG   2010-02-03  23.889999  24.450001  23.709999  23.790001  16.759001   \n",
       "KLAC  2010-02-03  28.950001  29.410000  28.719999  29.290001  17.435822   \n",
       "TMUS  2010-02-03  11.740000  12.420000  11.660000  12.160000  10.075135   \n",
       "\n",
       "          Volume       date  pct_change  cum_return  adj_close_shifted  \\\n",
       "Name                                                                     \n",
       "ZION  10280400.0 2010-02-03   -0.064792    0.396849          16.577236   \n",
       "LYV    2317700.0 2010-02-03    0.034813    0.303728          11.470000   \n",
       "HBAN  41303100.0 2010-02-03   -0.044625    0.279891           3.525512   \n",
       "DPZ     635000.0 2010-02-03   -0.003597    0.276498           9.607740   \n",
       "LEN    4385101.0 2010-02-03   -0.017523    0.253082          14.234322   \n",
       "...          ...        ...         ...         ...                ...   \n",
       "HWM     160238.0 2010-02-03   -0.010997   -0.188008          26.038321   \n",
       "AMD   14521900.0 2010-02-03   -0.012642   -0.194845           7.460000   \n",
       "AIG    4817500.0 2010-02-03   -0.007509   -0.204082          17.068958   \n",
       "KLAC   6157800.0 2010-02-03    0.006530   -0.206663          16.786964   \n",
       "TMUS   4264950.0 2010-02-03    0.028765   -0.237139           9.329443   \n",
       "\n",
       "      adj_change  \n",
       "Name              \n",
       "ZION    0.981550  \n",
       "LYV     1.036617  \n",
       "HBAN    0.983299  \n",
       "DPZ     0.980531  \n",
       "LEN     1.058594  \n",
       "...          ...  \n",
       "HWM     1.062032  \n",
       "AMD     1.046917  \n",
       "AIG     0.981841  \n",
       "KLAC    1.038652  \n",
       "TMUS    1.079929  \n",
       "\n",
       "[451 rows x 12 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Date</th>\n      <th>Open</th>\n      <th>High</th>\n      <th>Low</th>\n      <th>Close</th>\n      <th>Adj Close</th>\n      <th>Volume</th>\n      <th>date</th>\n      <th>pct_change</th>\n      <th>cum_return</th>\n      <th>adj_close_shifted</th>\n      <th>adj_change</th>\n    </tr>\n    <tr>\n      <th>Name</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>ZION</th>\n      <td>2010-02-03</td>\n      <td>19.770000</td>\n      <td>19.780001</td>\n      <td>18.459999</td>\n      <td>18.620001</td>\n      <td>16.271383</td>\n      <td>10280400.0</td>\n      <td>2010-02-03</td>\n      <td>-0.064792</td>\n      <td>0.396849</td>\n      <td>16.577236</td>\n      <td>0.981550</td>\n    </tr>\n    <tr>\n      <th>LYV</th>\n      <td>2010-02-03</td>\n      <td>11.410000</td>\n      <td>11.900000</td>\n      <td>11.410000</td>\n      <td>11.890000</td>\n      <td>11.890000</td>\n      <td>2317700.0</td>\n      <td>2010-02-03</td>\n      <td>0.034813</td>\n      <td>0.303728</td>\n      <td>11.470000</td>\n      <td>1.036617</td>\n    </tr>\n    <tr>\n      <th>HBAN</th>\n      <td>2010-02-03</td>\n      <td>4.880000</td>\n      <td>4.900000</td>\n      <td>4.630000</td>\n      <td>4.710000</td>\n      <td>3.466631</td>\n      <td>41303100.0</td>\n      <td>2010-02-03</td>\n      <td>-0.044625</td>\n      <td>0.279891</td>\n      <td>3.525512</td>\n      <td>0.983299</td>\n    </tr>\n    <tr>\n      <th>DPZ</th>\n      <td>2010-02-03</td>\n      <td>11.110000</td>\n      <td>11.200000</td>\n      <td>11.030000</td>\n      <td>11.080000</td>\n      <td>9.420688</td>\n      <td>635000.0</td>\n      <td>2010-02-03</td>\n      <td>-0.003597</td>\n      <td>0.276498</td>\n      <td>9.607740</td>\n      <td>0.980531</td>\n    </tr>\n    <tr>\n      <th>LEN</th>\n      <td>2010-02-03</td>\n      <td>16.096361</td>\n      <td>16.243855</td>\n      <td>15.644051</td>\n      <td>15.988201</td>\n      <td>15.068360</td>\n      <td>4385101.0</td>\n      <td>2010-02-03</td>\n      <td>-0.017523</td>\n      <td>0.253082</td>\n      <td>14.234322</td>\n      <td>1.058594</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>HWM</th>\n      <td>2010-02-03</td>\n      <td>30.584707</td>\n      <td>31.236881</td>\n      <td>30.202398</td>\n      <td>30.337332</td>\n      <td>27.653538</td>\n      <td>160238.0</td>\n      <td>2010-02-03</td>\n      <td>-0.010997</td>\n      <td>-0.188008</td>\n      <td>26.038321</td>\n      <td>1.062032</td>\n    </tr>\n    <tr>\n      <th>AMD</th>\n      <td>2010-02-03</td>\n      <td>7.830000</td>\n      <td>7.960000</td>\n      <td>7.760000</td>\n      <td>7.810000</td>\n      <td>7.810000</td>\n      <td>14521900.0</td>\n      <td>2010-02-03</td>\n      <td>-0.012642</td>\n      <td>-0.194845</td>\n      <td>7.460000</td>\n      <td>1.046917</td>\n    </tr>\n    <tr>\n      <th>AIG</th>\n      <td>2010-02-03</td>\n      <td>23.889999</td>\n      <td>24.450001</td>\n      <td>23.709999</td>\n      <td>23.790001</td>\n      <td>16.759001</td>\n      <td>4817500.0</td>\n      <td>2010-02-03</td>\n      <td>-0.007509</td>\n      <td>-0.204082</td>\n      <td>17.068958</td>\n      <td>0.981841</td>\n    </tr>\n    <tr>\n      <th>KLAC</th>\n      <td>2010-02-03</td>\n      <td>28.950001</td>\n      <td>29.410000</td>\n      <td>28.719999</td>\n      <td>29.290001</td>\n      <td>17.435822</td>\n      <td>6157800.0</td>\n      <td>2010-02-03</td>\n      <td>0.006530</td>\n      <td>-0.206663</td>\n      <td>16.786964</td>\n      <td>1.038652</td>\n    </tr>\n    <tr>\n      <th>TMUS</th>\n      <td>2010-02-03</td>\n      <td>11.740000</td>\n      <td>12.420000</td>\n      <td>11.660000</td>\n      <td>12.160000</td>\n      <td>10.075135</td>\n      <td>4264950.0</td>\n      <td>2010-02-03</td>\n      <td>0.028765</td>\n      <td>-0.237139</td>\n      <td>9.329443</td>\n      <td>1.079929</td>\n    </tr>\n  </tbody>\n</table>\n<p>451 rows × 12 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 100
    }
   ],
   "source": [
    "df0 = get_daily_j(0).set_index('Name') # i dont know why, but you have to set index to name for the groupby's to work\n",
    "df0['pct_change'] = df0['Adj Close'].groupby('Name').pct_change()\n",
    "df0['cum_return'] = (df0['pct_change']+1).groupby('Name').cumprod().fillna(1)-1\n",
    "df0['adj_close_shifted'] = df0['Adj Close'].groupby('Name').shift(3)#.bfill(0) # filing the value with the backfill TODO check this later\n",
    "df0['adj_change'] = df0['Adj Close']/df0['adj_close_shifted'] #.bfill(0) \n",
    "\n",
    "features = df0.groupby('Name').tail(1).sort_values(['cum_return'], ascending=False)\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# monthly_sp500['PYPL'].head() # TODO we should write about some of the issues of yahoofinance as a datasource, that should be sufficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Index(['ZION', 'LYV', 'HBAN', 'DPZ', 'LEN', 'KEY', 'ILMN', 'DHI', 'AAL',\n",
       "       'FITB'],\n",
       "      dtype='object', name='Name')"
      ]
     },
     "metadata": {},
     "execution_count": 102
    }
   ],
   "source": [
    "# this calculates the winner and loser tickers given n\n",
    "n = 50\n",
    "get_percents = lambda n: features.shape[0]//n+1 # this function gets us n percent number of tickers\n",
    "winner_tickers = features[:get_percents(n)].index\n",
    "winner_tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Index(['IP', 'LRCX', 'MU', 'MPWR', 'URI', 'HWM', 'AMD', 'AIG', 'KLAC', 'TMUS'], dtype='object', name='Name')"
      ]
     },
     "metadata": {},
     "execution_count": 103
    }
   ],
   "source": [
    "loser_tickers = features[-get_percents(n):].index\n",
    "loser_tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# equally weight\n",
    "weights = np.ones(get_percents(n))/(get_percents(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Date\n",
       "2010-01-04    0.000000\n",
       "2010-01-05    0.034084\n",
       "2010-01-06    0.059467\n",
       "2010-01-07    0.119671\n",
       "2010-01-08    0.119166\n",
       "2010-01-11    0.124764\n",
       "2010-01-12    0.135642\n",
       "2010-01-13    0.167968\n",
       "2010-01-14    0.192803\n",
       "2010-01-15    0.175896\n",
       "2010-01-19    0.186416\n",
       "2010-01-20    0.185581\n",
       "2010-01-21    0.191856\n",
       "2010-01-22    0.175227\n",
       "2010-01-25    0.199872\n",
       "2010-01-26    0.202174\n",
       "2010-01-27    0.223839\n",
       "2010-01-28    0.235507\n",
       "2010-01-29    0.235434\n",
       "2010-02-01    0.258644\n",
       "2010-02-02    0.293070\n",
       "2010-02-03    0.267778\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "execution_count": 105
    }
   ],
   "source": [
    "winner_df0 = df0.loc[winner_tickers.to_list()]\n",
    "winner_df0 = winner_df0.pivot_table(index='Date',columns='Name')['pct_change'].fillna(0) # \n",
    "winner_df0\n",
    "# winner_df0.dot(weights) # gives us the percent change of the portfolios\n",
    "winner_performance = (winner_df0.dot(weights)+1).cumprod() - 1 # gets cumulative return for a period\n",
    "winner_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      ".weights 226\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "             winners    losers\n",
       "Date                          \n",
       "2010-01-04  0.000000  0.000000\n",
       "2010-01-05  0.007279  0.000804\n",
       "2010-01-06  0.013506 -0.001028\n",
       "2010-01-07  0.025162 -0.000676\n",
       "2010-01-08  0.027712  0.002783\n",
       "2010-01-11  0.032591  0.003581\n",
       "2010-01-12  0.027366 -0.012536\n",
       "2010-01-13  0.038880 -0.003022\n",
       "2010-01-14  0.043425 -0.004658\n",
       "2010-01-15  0.034115 -0.018161\n",
       "2010-01-19  0.049060 -0.007063\n",
       "2010-01-20  0.042306 -0.019628\n",
       "2010-01-21  0.030291 -0.038098\n",
       "2010-01-22  0.013952 -0.062552\n",
       "2010-01-25  0.017791 -0.059261\n",
       "2010-01-26  0.015786 -0.064917\n",
       "2010-01-27  0.023182 -0.062596\n",
       "2010-01-28  0.017563 -0.079618\n",
       "2010-01-29  0.010996 -0.092202\n",
       "2010-02-01  0.026390 -0.077000\n",
       "2010-02-02  0.043323 -0.067243\n",
       "2010-02-03  0.036727 -0.073746"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>winners</th>\n      <th>losers</th>\n    </tr>\n    <tr>\n      <th>Date</th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2010-01-04</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>2010-01-05</th>\n      <td>0.007279</td>\n      <td>0.000804</td>\n    </tr>\n    <tr>\n      <th>2010-01-06</th>\n      <td>0.013506</td>\n      <td>-0.001028</td>\n    </tr>\n    <tr>\n      <th>2010-01-07</th>\n      <td>0.025162</td>\n      <td>-0.000676</td>\n    </tr>\n    <tr>\n      <th>2010-01-08</th>\n      <td>0.027712</td>\n      <td>0.002783</td>\n    </tr>\n    <tr>\n      <th>2010-01-11</th>\n      <td>0.032591</td>\n      <td>0.003581</td>\n    </tr>\n    <tr>\n      <th>2010-01-12</th>\n      <td>0.027366</td>\n      <td>-0.012536</td>\n    </tr>\n    <tr>\n      <th>2010-01-13</th>\n      <td>0.038880</td>\n      <td>-0.003022</td>\n    </tr>\n    <tr>\n      <th>2010-01-14</th>\n      <td>0.043425</td>\n      <td>-0.004658</td>\n    </tr>\n    <tr>\n      <th>2010-01-15</th>\n      <td>0.034115</td>\n      <td>-0.018161</td>\n    </tr>\n    <tr>\n      <th>2010-01-19</th>\n      <td>0.049060</td>\n      <td>-0.007063</td>\n    </tr>\n    <tr>\n      <th>2010-01-20</th>\n      <td>0.042306</td>\n      <td>-0.019628</td>\n    </tr>\n    <tr>\n      <th>2010-01-21</th>\n      <td>0.030291</td>\n      <td>-0.038098</td>\n    </tr>\n    <tr>\n      <th>2010-01-22</th>\n      <td>0.013952</td>\n      <td>-0.062552</td>\n    </tr>\n    <tr>\n      <th>2010-01-25</th>\n      <td>0.017791</td>\n      <td>-0.059261</td>\n    </tr>\n    <tr>\n      <th>2010-01-26</th>\n      <td>0.015786</td>\n      <td>-0.064917</td>\n    </tr>\n    <tr>\n      <th>2010-01-27</th>\n      <td>0.023182</td>\n      <td>-0.062596</td>\n    </tr>\n    <tr>\n      <th>2010-01-28</th>\n      <td>0.017563</td>\n      <td>-0.079618</td>\n    </tr>\n    <tr>\n      <th>2010-01-29</th>\n      <td>0.010996</td>\n      <td>-0.092202</td>\n    </tr>\n    <tr>\n      <th>2010-02-01</th>\n      <td>0.026390</td>\n      <td>-0.077000</td>\n    </tr>\n    <tr>\n      <th>2010-02-02</th>\n      <td>0.043323</td>\n      <td>-0.067243</td>\n    </tr>\n    <tr>\n      <th>2010-02-03</th>\n      <td>0.036727</td>\n      <td>-0.073746</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 106
    }
   ],
   "source": [
    "def get_cum_return(data):\n",
    "    '''gets cumulative return based on adjusted closing price of all tickers in input dataframe'''\n",
    "    df_ = data.set_index('Name')\n",
    "\n",
    "    # df_['pct_change'] = df_['Adj Close'].groupby('Name').pct_change()\n",
    "    df_['pct_change'] = df_['Adj Close'].groupby('Name').pct_change()\n",
    "    df_['cum_return'] = (df_['pct_change']+1).groupby('Name').cumprod().fillna(1)-1\n",
    "    df_['adj_close_shifted'] = df_['Adj Close'].groupby('Name').shift(3)#.bfill(0) # filing the value with the backfill TODO check this later\n",
    "    df_['adj_change'] = df_['Adj Close']/df_['adj_close_shifted'] #.bfill(0) \n",
    "    df_['adj_close_shifted'] = df_['Adj Close'].groupby('Name').shift(3)#.bfill(0) # filing the value with the backfill TODO check this later\n",
    "    df_['adj_change'] = df_['Adj Close']/df_['adj_close_shifted'] #.bfill(0) \n",
    "    \n",
    "    return df_\n",
    "\n",
    "def get_percents(n): \n",
    "    '''this function gets us n percent number of tickers'''\n",
    "    return features.shape[0]//n+1\n",
    "\n",
    "def get_portfolios(period=0, n=50):   \n",
    "    '''returns the portfolios we want from a particular period'''\n",
    "    ## Gets the portfolio(s) in question we want to look at\n",
    "    data = get_cum_return(get_daily_j(period))\n",
    "    features = data.groupby('Name').tail(1).sort_values(['cum_return'], ascending=False) # sorts tickers by cumulative return\n",
    "\n",
    "    winner_tickers = features[:get_percents(n)].index # we get the top/bottom n percent tickers\n",
    "    loser_tickers = features[-get_percents(n):].index # only winner and loser portfolios for now, we could expand later\n",
    "    # TODO add more portfolios if we have time\n",
    "    # print(len(winner_tickers), len(loser_tickers))\n",
    "    print('.', end='')\n",
    "    return (winner_tickers.to_list(), loser_tickers.to_list())\n",
    "\n",
    "def get_portfolio_performance(period=0, n=2, weights=None, hold=False):\n",
    "    '''Gets all portfolios' cumulative return performance based on n period \n",
    "    keyword args:\n",
    "    period      -- the nth period of data we are looking at, default 0\n",
    "    portfolio   -- the particular type of portfolio we want to be looking at (winner or loser, etc) default winner/momentum\n",
    "    n           -- the percentage of tickers we want to be looking at\n",
    "    weights     -- how to weight the portfolio values. if unspecified (None) we assume equal weighting in the portfolio\n",
    "    hold        -- if true returns the performance evaluation of the holding period k, else returns the performance of the observation period j\n",
    "    '''\n",
    "    winner_tickers, loser_tickers = get_portfolios(period, n)\n",
    "\n",
    "    ## evaluates the performance of portfolios on either hold or observational data\n",
    "    eval_df = get_cum_return(get_k_df(period)) if hold else get_cum_return(get_daily_j(period))# we get the return from the hold period\n",
    "    \n",
    "    weights = np.ones(get_percents(n))/(get_percents(n)) if not weights else weights # set weights\n",
    "    print('weights', len(weights))\n",
    "    winner_eval = eval_df.loc[winner_tickers] # first the winners\n",
    "    winner_eval = winner_eval.pivot_table(index='Date',columns='Name')['pct_change'].fillna(0)\n",
    "    winner_performance = (winner_eval.dot(weights)+1).cumprod() - 1 # cumulative return\n",
    "\n",
    "    loser_eval = eval_df.loc[loser_tickers] # then the losers\n",
    "    loser_eval = loser_eval.pivot_table(index='Date',columns='Name')['pct_change'].fillna(0)\n",
    "    loser_performance = (loser_eval.dot(weights)+1).cumprod() - 1 # cumulative return\n",
    "\n",
    "    output = pd.DataFrame({'winners': winner_performance, 'losers': loser_performance})\n",
    "    return output\n",
    "get_portfolio_performance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "."
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(['CNC', 'PVH', 'INCY', 'AIG', 'PXD', 'C', 'LVS', 'PFG', 'HST', 'CBRE'],\n",
       " ['MOS', 'DVN', 'NRG', 'ABMD', 'BSX', 'IT', 'COG', 'STX', 'LDOS', 'CF'])"
      ]
     },
     "metadata": {},
     "execution_count": 107
    }
   ],
   "source": [
    "get_portfolios(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10/500\n",
    "# pf = pd.DataFrame({'idx':timedf.index})\n",
    "# pf['mom'], pf['rev'] = zip(*pf['idx'].map(get_portfolios))\n",
    "# pf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "...................................................................."
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "     j_start   midpoint      k_end  \\\n",
       "0 2010-01-04 2010-02-03 2010-04-04   \n",
       "1 2010-03-05 2010-04-04 2010-06-03   \n",
       "2 2010-05-04 2010-06-03 2010-08-02   \n",
       "3 2010-07-03 2010-08-02 2010-10-01   \n",
       "4 2010-09-01 2010-10-01 2010-11-30   \n",
       "\n",
       "                                                 mom  \\\n",
       "0  [ZION, LYV, HBAN, DPZ, LEN, KEY, ILMN, DHI, AA...   \n",
       "1  [CNC, PVH, INCY, AIG, PXD, C, LVS, PFG, HST, C...   \n",
       "2  [AAL, DAL, UHS, ALK, NTAP, TMUS, AAP, NFLX, UA...   \n",
       "3  [URI, WY, CBRE, AAL, TXT, TSLA, IPG, RCL, UAL, F]   \n",
       "4  [KMX, ULTA, URI, BBY, PVH, UAA, INCY, AMD, WBA...   \n",
       "\n",
       "                                                 rev  \n",
       "0  [IP, LRCX, MU, MPWR, URI, HWM, AMD, AIG, KLAC,...  \n",
       "1  [MOS, DVN, NRG, ABMD, BSX, IT, COG, STX, LDOS,...  \n",
       "2  [LEN, MCO, DHI, LYB, MA, LYV, BR, PHM, HAL, BKNG]  \n",
       "3  [HSY, NLOK, MCK, CBOE, AKAM, TMO, BSX, MU, WDC...  \n",
       "4  [JCI, AAL, LNC, CPRT, CRM, MPWR, ETR, CBOE, MT...  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>j_start</th>\n      <th>midpoint</th>\n      <th>k_end</th>\n      <th>mom</th>\n      <th>rev</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2010-01-04</td>\n      <td>2010-02-03</td>\n      <td>2010-04-04</td>\n      <td>[ZION, LYV, HBAN, DPZ, LEN, KEY, ILMN, DHI, AA...</td>\n      <td>[IP, LRCX, MU, MPWR, URI, HWM, AMD, AIG, KLAC,...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2010-03-05</td>\n      <td>2010-04-04</td>\n      <td>2010-06-03</td>\n      <td>[CNC, PVH, INCY, AIG, PXD, C, LVS, PFG, HST, C...</td>\n      <td>[MOS, DVN, NRG, ABMD, BSX, IT, COG, STX, LDOS,...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2010-05-04</td>\n      <td>2010-06-03</td>\n      <td>2010-08-02</td>\n      <td>[AAL, DAL, UHS, ALK, NTAP, TMUS, AAP, NFLX, UA...</td>\n      <td>[LEN, MCO, DHI, LYB, MA, LYV, BR, PHM, HAL, BKNG]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2010-07-03</td>\n      <td>2010-08-02</td>\n      <td>2010-10-01</td>\n      <td>[URI, WY, CBRE, AAL, TXT, TSLA, IPG, RCL, UAL, F]</td>\n      <td>[HSY, NLOK, MCK, CBOE, AKAM, TMO, BSX, MU, WDC...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2010-09-01</td>\n      <td>2010-10-01</td>\n      <td>2010-11-30</td>\n      <td>[KMX, ULTA, URI, BBY, PVH, UAA, INCY, AMD, WBA...</td>\n      <td>[JCI, AAL, LNC, CPRT, CRM, MPWR, ETR, CBOE, MT...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 109
    }
   ],
   "source": [
    "portfolios = {'mom':{}, 'rev':{}}\n",
    "for i in timedf.index:\n",
    "    # print(i)\n",
    "    mom, rev = get_portfolios(i)\n",
    "    portfolios['mom'][i] = mom\n",
    "    portfolios['rev'][i] = rev\n",
    "# portfolios['mom'][0]\n",
    "pf = timedf.merge(pd.DataFrame(portfolios), left_index=True, right_index=True)\n",
    "pf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-110-eea725d9058c>, line 6)",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-110-eea725d9058c>\"\u001b[1;36m, line \u001b[1;32m6\u001b[0m\n\u001b[1;33m    value = portfolio_df[].values\u001b[0m\n\u001b[1;37m                         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def get_portfolio_pct(lst, date, weights=None):\n",
    "    '''given list of tickers, generates a EQUAL WEIGHTED INDEX portfolio and aggregates their performance'''\n",
    "    portfolio_df = df[df['Name'].isin(lst)] #filters df by lst\n",
    "    portfolio_df = get_cum_return(portfolio_df)[['Date','pct_change']] # we get cum return\n",
    "    portfolio_df = portfolio_df.groupby('Date').mean()\n",
    "    value = portfolio_df[pd.to_datetime(portfolio_df.index)==pd.to_datetime(date)].values\n",
    "    # print('.', end='')\n",
    "    return 0 if (value.shape[0]==0 or np.isnan(value[0,0])) else value[0,0] # will need to spend time staring at the output to see if most columns are good\n",
    "    # im thinking the only good value we can get out of this is percent change and maybe cum_return, ohlc columns are garbage\n",
    "# temp = get_portfolio_pct(['NFLX', 'LVS', 'URI', 'CMG', 'FFIV'], dpf.Date[0])\n",
    "# temp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpf = pd.DataFrame({'Date':pd.date_range(df.Date.min(), df.Date.max(), freq=\"D\")}).merge(pf[['k_end', 'mom', 'rev']].reset_index(), how='left', left_on='Date', right_on='k_end').bfill().ffill()\n",
    "dpf['mom_pct_change'] = dpf.apply(lambda x: get_portfolio_pct(x.mom, x.Date), axis=1)\n",
    "dpf['rev_pct_change'] = dpf.apply(lambda x: get_portfolio_pct(x.rev, x.Date), axis=1)\n",
    "dpf['index'] = dpf['index'].astype(int)\n",
    "dpf = dpf.drop(['mom', 'rev', 'k_end'], axis=1)\n",
    "dpf['mom_return'] = ((dpf['mom_pct_change']+1).cumprod()-1) * 100\n",
    "dpf['rev_return'] = ((dpf['rev_pct_change']+1).cumprod()-1) * 100\n",
    "dpf[['mom_return', 'rev_return', 'Date']].set_index('Date').plot(figsize=(12,8))\n",
    "dpf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spc = pdr.get_data_yahoo(\"^GSPC\", start=\"2009-10-01\")\n",
    "spc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.ticker as mtick\n",
    "\n",
    "fig = plt.figure(1, (12,8))\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "fmt = '%.0f%%' # Format you want the ticks, e.g. '40%'\n",
    "xticks = mtick.FormatStrFormatter(fmt)\n",
    "ax.yaxis.set_major_formatter(xticks)\n",
    "spc['pctc'] = spc['Adj Close'].pct_change()\n",
    "spc['cumprod'] =((spc['pctc']+1).cumprod()-1)*100\n",
    "sp500 = spc.merge(dpf, how='left', left_index=True, right_on='Date')\n",
    "sp500[['Date', 'mom_return', 'rev_return','cumprod']].set_index('Date').plot.area(ax=ax,stacked=False,figsize=(12,8)) # plotted\n",
    "ax.legend([\"Momentum\", \"Reversal\", \"S&P500\"])\n",
    "# fig.savefig('performance.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp500 = sp500.drop(['mom_return', 'rev_return'], axis=1) # mom_retunr and rev_return, beware of use\n",
    "sp500 = sp500.reset_index(drop=True).set_index('Date')\n",
    "sp500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sp500.ta.cci(length=4,append=True)\n",
    "sp500.ta.kdj(length=4,append=True)\n",
    "sp500.ta.rsi(length=4,append=True)\n",
    "sp500.ta.bop(length=4,append=True)\n",
    "sp500.ta.willr(length=4,append=True)\n",
    "sp500.ta.pdist(length=4,append=True)\n",
    "sp500.ta.kc(length=4,append=True)\n",
    "sp500.ta.adx(length=4,append=True)\n",
    "sp500.ta.qstick(length=4,append=True)\n",
    "sp500.ta.roc(length=4,append=True)\n",
    "# sp500.ta.ao(length=4,append=True)\n",
    "# sp500.ta.macd(fast=4,slow=8,append=True)\n",
    "sp500.ta.stdev(length=4,append=True)\n",
    "sp500.ta.pvol(append=True)\n",
    "sp500.ta.efi(length=4, append=True)\n",
    "sp500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp500.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp500[['CCI_4_0.015',\n",
    "       'K_4_3', 'D_4_3', 'J_4_3', 'RSI_4', 'BOP', 'WILLR_4', 'PDIST',\n",
    "       'KCLe_4_2', 'KCBe_4_2', 'KCUe_4_2', 'ADX_4', 'DMP_4', 'DMN_4', 'QS_4',\n",
    "       'ROC_4', 'STDEV_4', 'PVOL', 'EFI_4']][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.date_range(df.Date.min(), df.Date.max())\n",
    "sp500 = sp500[sp500.index.isin(pd.date_range(timedf.j_start.min(), timedf.k_end.max()))]\n",
    "sp500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timedf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp500.to_csv('daily_j20_k40.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "'date' is both an index level and a column label, which is ambiguous.",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-51-90838a96125a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0msp500\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Date'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msp500\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msp500\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mget_daily_j\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-9-fec5ea69f343>\u001b[0m in \u001b[0;36mget_daily_j\u001b[1;34m(id)\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mdf_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mdf_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'date'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Date'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mdf_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjtimes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'inner'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'date'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mid\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdf_\u001b[0m \u001b[1;31m# this filters out periods that dont end prettily\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.virtualenvs\\dsc190-FVH1D0fH\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mmerge\u001b[1;34m(self, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m   8203\u001b[0m             \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   8204\u001b[0m             \u001b[0mindicator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mindicator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 8205\u001b[1;33m             \u001b[0mvalidate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   8206\u001b[0m         )\n\u001b[0;32m   8207\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.virtualenvs\\dsc190-FVH1D0fH\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36mmerge\u001b[1;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m     85\u001b[0m         \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m         \u001b[0mindicator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mindicator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m         \u001b[0mvalidate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m     )\n\u001b[0;32m     89\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.virtualenvs\\dsc190-FVH1D0fH\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, left, right, how, on, left_on, right_on, axis, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m    666\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mright_join_keys\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    667\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin_names\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 668\u001b[1;33m         ) = self._get_merge_keys()\n\u001b[0m\u001b[0;32m    669\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    670\u001b[0m         \u001b[1;31m# validate the merge keys dtypes. We may need to coerce\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.virtualenvs\\dsc190-FVH1D0fH\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m_get_merge_keys\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1044\u001b[0m                         \u001b[0mright_keys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1045\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mlk\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1046\u001b[1;33m                         \u001b[0mleft_keys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_label_or_level_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1047\u001b[0m                         \u001b[0mjoin_names\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1048\u001b[0m                     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.virtualenvs\\dsc190-FVH1D0fH\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_get_label_or_level_values\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1676\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1677\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_label_reference\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1678\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_label_or_level_ambiguity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1679\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mother_axes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1680\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_level_reference\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.virtualenvs\\dsc190-FVH1D0fH\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_check_label_or_level_ambiguity\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1635\u001b[0m                 \u001b[1;34mf\"{label_article} {label_type} label, which is ambiguous.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1636\u001b[0m             )\n\u001b[1;32m-> 1637\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1638\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1639\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mfinal\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: 'date' is both an index level and a column label, which is ambiguous."
     ]
    }
   ],
   "source": [
    "sp500 = sp500.fillna(0)\n",
    "# sp500['date'] = sp500.index\n",
    "sp500['Date'] = sp500.index\n",
    "df = sp500\n",
    "get_daily_j()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'get_j_df' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-45-ac80b02caff8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mper0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_j_df\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mper0\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'get_j_df' is not defined"
     ]
    }
   ],
   "source": [
    "per0 = get_j_df(0)\n",
    "per0.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "KeyError",
     "evalue": "'mom_pct_change'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\.virtualenvs\\dsc190-FVH1D0fH\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3079\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3080\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3081\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'mom_pct_change'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-89-f33144f47162>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mper0\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'mom_cum_ret'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mper0\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'mom_pct_change'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcumprod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mper0\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'rev_cum_ret'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mper0\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'rev_pct_change'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcumprod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mper0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.virtualenvs\\dsc190-FVH1D0fH\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3022\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3023\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3024\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3025\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3026\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.virtualenvs\\dsc190-FVH1D0fH\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3080\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3081\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3082\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3083\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3084\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'mom_pct_change'"
     ]
    }
   ],
   "source": [
    "per0['mom_cum_ret'] = (per0['mom_pct_change']+1).cumprod()-1\n",
    "per0['rev_cum_ret'] = (per0['rev_pct_change']+1).cumprod()-1\n",
    "per0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "leptokurtic distribution\n",
    "the only things we can do is buy, hold, or leave for rev or momentum\n",
    "<br><br>choices:\n",
    "- momentum\n",
    "- reversal\n",
    "- neither"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                   Open         High          Low        Close    Adj Close  \\\n",
       "Date                                                                          \n",
       "2010-01-01  1116.560059  1150.449951  1071.589966  1073.869995  1073.869995   \n",
       "2010-02-01  1073.890015  1112.420044  1044.500000  1104.489990  1104.489990   \n",
       "2010-03-01  1105.359985  1180.689941  1105.359985  1169.430054  1169.430054   \n",
       "2010-04-01  1171.229980  1219.800049  1170.689941  1186.689941  1186.689941   \n",
       "2010-05-01  1188.579956  1205.130005  1040.780029  1089.410034  1089.410034   \n",
       "...                 ...          ...          ...          ...          ...   \n",
       "2020-12-01  3645.870117  3760.199951  3633.399902  3756.070068  3756.070068   \n",
       "2021-01-01  3764.610107  3870.899902  3662.709961  3714.239990  3714.239990   \n",
       "2021-02-01  3731.169922  3950.429932  3725.620117  3811.149902  3811.149902   \n",
       "2021-03-01  3842.510010  3914.500000  3723.340088  3841.939941  3841.939941   \n",
       "2021-03-08  3844.389893  3881.060059  3819.250000  3821.350098  3821.350098   \n",
       "\n",
       "                  Volume      pctc     cumprod  \n",
       "Date                                            \n",
       "2010-01-01   90947580000       NaN         NaN  \n",
       "2010-02-01   84561340000  0.028514    2.851369  \n",
       "2010-03-01  103683550000  0.058796    8.898662  \n",
       "2010-04-01  116741910000  0.014759   10.505922  \n",
       "2010-05-01  127662780000 -0.081976    1.447106  \n",
       "...                  ...       ...         ...  \n",
       "2020-12-01   96056410000  0.037121  249.769533  \n",
       "2021-01-01  105548790000 -0.011137  245.874268  \n",
       "2021-02-01   98596960000  0.026091  254.898630  \n",
       "2021-03-01   30700830000  0.008079  257.765834  \n",
       "2021-03-08    2991626851 -0.005359  255.848484  \n",
       "\n",
       "[136 rows x 8 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Open</th>\n      <th>High</th>\n      <th>Low</th>\n      <th>Close</th>\n      <th>Adj Close</th>\n      <th>Volume</th>\n      <th>pctc</th>\n      <th>cumprod</th>\n    </tr>\n    <tr>\n      <th>Date</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2010-01-01</th>\n      <td>1116.560059</td>\n      <td>1150.449951</td>\n      <td>1071.589966</td>\n      <td>1073.869995</td>\n      <td>1073.869995</td>\n      <td>90947580000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2010-02-01</th>\n      <td>1073.890015</td>\n      <td>1112.420044</td>\n      <td>1044.500000</td>\n      <td>1104.489990</td>\n      <td>1104.489990</td>\n      <td>84561340000</td>\n      <td>0.028514</td>\n      <td>2.851369</td>\n    </tr>\n    <tr>\n      <th>2010-03-01</th>\n      <td>1105.359985</td>\n      <td>1180.689941</td>\n      <td>1105.359985</td>\n      <td>1169.430054</td>\n      <td>1169.430054</td>\n      <td>103683550000</td>\n      <td>0.058796</td>\n      <td>8.898662</td>\n    </tr>\n    <tr>\n      <th>2010-04-01</th>\n      <td>1171.229980</td>\n      <td>1219.800049</td>\n      <td>1170.689941</td>\n      <td>1186.689941</td>\n      <td>1186.689941</td>\n      <td>116741910000</td>\n      <td>0.014759</td>\n      <td>10.505922</td>\n    </tr>\n    <tr>\n      <th>2010-05-01</th>\n      <td>1188.579956</td>\n      <td>1205.130005</td>\n      <td>1040.780029</td>\n      <td>1089.410034</td>\n      <td>1089.410034</td>\n      <td>127662780000</td>\n      <td>-0.081976</td>\n      <td>1.447106</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2020-12-01</th>\n      <td>3645.870117</td>\n      <td>3760.199951</td>\n      <td>3633.399902</td>\n      <td>3756.070068</td>\n      <td>3756.070068</td>\n      <td>96056410000</td>\n      <td>0.037121</td>\n      <td>249.769533</td>\n    </tr>\n    <tr>\n      <th>2021-01-01</th>\n      <td>3764.610107</td>\n      <td>3870.899902</td>\n      <td>3662.709961</td>\n      <td>3714.239990</td>\n      <td>3714.239990</td>\n      <td>105548790000</td>\n      <td>-0.011137</td>\n      <td>245.874268</td>\n    </tr>\n    <tr>\n      <th>2021-02-01</th>\n      <td>3731.169922</td>\n      <td>3950.429932</td>\n      <td>3725.620117</td>\n      <td>3811.149902</td>\n      <td>3811.149902</td>\n      <td>98596960000</td>\n      <td>0.026091</td>\n      <td>254.898630</td>\n    </tr>\n    <tr>\n      <th>2021-03-01</th>\n      <td>3842.510010</td>\n      <td>3914.500000</td>\n      <td>3723.340088</td>\n      <td>3841.939941</td>\n      <td>3841.939941</td>\n      <td>30700830000</td>\n      <td>0.008079</td>\n      <td>257.765834</td>\n    </tr>\n    <tr>\n      <th>2021-03-08</th>\n      <td>3844.389893</td>\n      <td>3881.060059</td>\n      <td>3819.250000</td>\n      <td>3821.350098</td>\n      <td>3821.350098</td>\n      <td>2991626851</td>\n      <td>-0.005359</td>\n      <td>255.848484</td>\n    </tr>\n  </tbody>\n</table>\n<p>136 rows × 8 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 60
    }
   ],
   "source": [
    "spc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "      j_start   midpoint      k_end\n",
       "0  2010-02-01 2010-11-01 2011-05-01\n",
       "1  2010-08-01 2011-05-01 2011-11-01\n",
       "2  2011-02-01 2011-11-01 2012-05-01\n",
       "3  2011-08-01 2012-05-01 2012-11-01\n",
       "4  2012-02-01 2012-11-01 2013-05-01\n",
       "5  2012-08-01 2013-05-01 2013-11-01\n",
       "6  2013-02-01 2013-11-01 2014-05-01\n",
       "7  2013-08-01 2014-05-01 2014-11-01\n",
       "8  2014-02-01 2014-11-01 2015-05-01\n",
       "9  2014-08-01 2015-05-01 2015-11-01\n",
       "10 2015-02-01 2015-11-01 2016-05-01\n",
       "11 2015-08-01 2016-05-01 2016-11-01\n",
       "12 2016-02-01 2016-11-01 2017-05-01\n",
       "13 2016-08-01 2017-05-01 2017-11-01\n",
       "14 2017-02-01 2017-11-01 2018-05-01\n",
       "15 2017-08-01 2018-05-01 2018-11-01\n",
       "16 2018-02-01 2018-11-01 2019-05-01\n",
       "17 2018-08-01 2019-05-01 2019-11-01\n",
       "18 2019-02-01 2019-11-01 2020-05-01\n",
       "19 2019-08-01 2020-05-01 2020-11-01\n",
       "20 2020-02-01 2020-11-01 2021-05-01"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>j_start</th>\n      <th>midpoint</th>\n      <th>k_end</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2010-02-01</td>\n      <td>2010-11-01</td>\n      <td>2011-05-01</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2010-08-01</td>\n      <td>2011-05-01</td>\n      <td>2011-11-01</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2011-02-01</td>\n      <td>2011-11-01</td>\n      <td>2012-05-01</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2011-08-01</td>\n      <td>2012-05-01</td>\n      <td>2012-11-01</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2012-02-01</td>\n      <td>2012-11-01</td>\n      <td>2013-05-01</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>2012-08-01</td>\n      <td>2013-05-01</td>\n      <td>2013-11-01</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>2013-02-01</td>\n      <td>2013-11-01</td>\n      <td>2014-05-01</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>2013-08-01</td>\n      <td>2014-05-01</td>\n      <td>2014-11-01</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>2014-02-01</td>\n      <td>2014-11-01</td>\n      <td>2015-05-01</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>2014-08-01</td>\n      <td>2015-05-01</td>\n      <td>2015-11-01</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>2015-02-01</td>\n      <td>2015-11-01</td>\n      <td>2016-05-01</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>2015-08-01</td>\n      <td>2016-05-01</td>\n      <td>2016-11-01</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>2016-02-01</td>\n      <td>2016-11-01</td>\n      <td>2017-05-01</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>2016-08-01</td>\n      <td>2017-05-01</td>\n      <td>2017-11-01</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>2017-02-01</td>\n      <td>2017-11-01</td>\n      <td>2018-05-01</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>2017-08-01</td>\n      <td>2018-05-01</td>\n      <td>2018-11-01</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>2018-02-01</td>\n      <td>2018-11-01</td>\n      <td>2019-05-01</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>2018-08-01</td>\n      <td>2019-05-01</td>\n      <td>2019-11-01</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>2019-02-01</td>\n      <td>2019-11-01</td>\n      <td>2020-05-01</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>2019-08-01</td>\n      <td>2020-05-01</td>\n      <td>2020-11-01</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>2020-02-01</td>\n      <td>2020-11-01</td>\n      <td>2021-05-01</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 61
    }
   ],
   "source": [
    "timedf[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.4 64-bit ('dsc190': pipenv)",
   "metadata": {
    "interpreter": {
     "hash": "e8c7d0e4fde2d427e9a5fa7c228e5b3fde1b07bcd87cbc5bf40713027eb29d09"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}